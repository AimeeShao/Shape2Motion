{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "import math\n",
        "from datetime import datetime\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import socket\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
        "sys.path.append(BASE_DIR)\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
        "import provider\n",
        "import tf_util\n",
        "import scipy.io as sio\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
        "parser.add_argument('--model', default='model2', help='Model name [default: model]')\n",
        "parser.add_argument('--stage_3_log_dir', default='stage_3_log', help='Log dir [default: log]')\n",
        "parser.add_argument('--num_point', type=int, default=4096, help='Point Number [default: 2048]')\n",
        "parser.add_argument('--max_epoch', type=int, default=101, help='Epoch to run [default: 201]')\n",
        "parser.add_argument('--batch_size', type=int, default=8, help='Batch Size during training [default: 32]')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
        "parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
        "parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')\n",
        "parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.7]')\n",
        "FLAGS = parser.parse_args()\n",
        "\n",
        "EPOCH_CNT = 0\n",
        "BATCH_SIZE = FLAGS.batch_size\n",
        "NUM_POINT = FLAGS.num_point\n",
        "MAX_EPOCH = FLAGS.max_epoch\n",
        "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
        "GPU_INDEX = FLAGS.gpu\n",
        "MOMENTUM = FLAGS.momentum\n",
        "OPTIMIZER = FLAGS.optimizer\n",
        "DECAY_STEP = FLAGS.decay_step\n",
        "DECAY_RATE = FLAGS.decay_rate\n",
        "\n",
        "MODEL = importlib.import_module(FLAGS.model) # import network module\n",
        "MODEL_FILE = os.path.join(ROOT_DIR, 'models', FLAGS.model+'.py')\n",
        "LOG_DIR = FLAGS.stage_3_log_dir\n",
        "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
        "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
        "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
        "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99\n",
        "\n",
        "HOSTNAME = socket.gethostname()\n",
        "\n",
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)\n",
        "\n",
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate        \n",
        "\n",
        "def get_bn_decay(batch):\n",
        "    bn_momentum = tf.train.exponential_decay(\n",
        "                      BN_INIT_DECAY,\n",
        "                      batch*BATCH_SIZE,\n",
        "                      BN_DECAY_DECAY_STEP,\n",
        "                      BN_DECAY_DECAY_RATE,\n",
        "                      staircase=True)\n",
        "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "    return bn_decay\n",
        "\n",
        "def train():\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
        "            print('stage_1')\n",
        "            pointclouds_pl,proposal_pl,dof_regression_pl,field_pl=MODEL.placeholder_inputs_stage_3(BATCH_SIZE,NUM_POINT)\n",
        "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "            # Note the global_step=batch parameter to minimize. \n",
        "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "            batch = tf.Variable(0)\n",
        "            bn_decay = get_bn_decay(batch)\n",
        "            tf.summary.scalar('bn_decay', bn_decay)\n",
        "            print \"--- Get model and loss\"\n",
        "            # Get model and loss \n",
        "            pred_proposal,pred_dof_regression = MODEL.get_stage_3(pointclouds_pl,field_pl, is_training_pl,bn_decay=bn_decay)\n",
        "            task_1_acc,iou,loss1,loss2,loss = MODEL.get_stage_3_loss(pred_proposal,pred_dof_regression,proposal_pl,dof_regression_pl)\n",
        "            tf.summary.scalar('loss', loss)\n",
        "\n",
        "            print(\"--- Get training operator\")\n",
        "            # Get training operator\n",
        "            learning_rate = get_learning_rate(batch)\n",
        "            tf.summary.scalar('learning_rate', learning_rate)\n",
        "            if OPTIMIZER == 'momentum':\n",
        "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "            elif OPTIMIZER == 'adam':\n",
        "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "            train_op = optimizer.minimize(loss, global_step=batch)\n",
        "            variables = tf.contrib.framework.get_variables_to_restore()\n",
        "            print(\"variables\")\n",
        "            for v in variables:\n",
        "                print(v)\n",
        "            # Add ops to save and restore all the variables.\n",
        "            saver = tf.train.Saver(max_to_keep=100)\n",
        "       \n",
        "        # Create a session\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        config.allow_soft_placement = True\n",
        "        config.log_device_placement = False\n",
        "        sess = tf.Session(config=config)\n",
        "\n",
        "        # Add summary writers\n",
        "        merged = tf.summary.merge_all()\n",
        "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
        "\n",
        "\n",
        "        # Init variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        ops = {'pointclouds_pl': pointclouds_pl,\n",
        "               'proposal_pl': proposal_pl,\n",
        "               'dof_regression_pl': dof_regression_pl,\n",
        "               'field_pl': field_pl,\n",
        "               'pred_proposal': pred_proposal,\n",
        "               'pred_dof_regression': pred_dof_regression,\n",
        "               'is_training_pl': is_training_pl,\n",
        "               'loss': loss,\n",
        "               'loss1':loss1,\n",
        "               'loss2':loss2,\n",
        "               'task_1_acc':task_1_acc,\n",
        "               'iou':iou,\n",
        "               'train_op': train_op,\n",
        "               'merged': merged,\n",
        "               'step': batch}\n",
        "        for epoch in range(MAX_EPOCH):\n",
        "            log_string('**** TRAIN EPOCH %03d ****' % (epoch))\n",
        "            sys.stdout.flush()\n",
        "            train_one_epoch_stage_3(sess,ops,train_writer)\n",
        "            # Save the variables to disk.\n",
        "            if epoch % 2 == 0:\n",
        "                model_ccc_path = \"model\"+str(epoch)+\".ckpt\"\n",
        "                save_path = saver.save(sess, os.path.join(LOG_DIR, model_ccc_path))\n",
        "                log_string(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "def train_one_epoch_stage_3(sess, ops, train_writer):\n",
        "    is_training = True\n",
        "    permutation = np.random.permutation(328)\n",
        "    for i in range(328/4):\n",
        "        load_data_start_time = time.time();\n",
        "        loadpath = './train_data_stage_3/train_data_stage_3_data_'+str(permutation[i*4]+1)+'.mat'\n",
        "        train_data = sio.loadmat(loadpath)['Training_data']\n",
        "        load_data_duration = time.time() - load_data_start_time\n",
        "        log_string('\\t%s: %s load time: %f' % (datetime.now(),loadpath,load_data_duration))\n",
        "        for j in range(3):\n",
        "            temp_load_data_start_time = time.time();\n",
        "            temp_loadpath = './train_data_stage_3/train_data_stage_3_data_'+str(permutation[i*4+j+1]+1)+'.mat'\n",
        "            temp_train_data = sio.loadmat(temp_loadpath)['Training_data']\n",
        "            temp_load_data_duration = time.time() - temp_load_data_start_time\n",
        "            log_string('\\t%s: %s load time: %f' % (datetime.now(),temp_loadpath,temp_load_data_duration))\n",
        "            train_data = np.concatenate((train_data,temp_train_data),axis = 0)\n",
        "            print(train_data.shape)\n",
        "        num_data = train_data.shape[0]\n",
        "        num_batch = num_data // BATCH_SIZE\n",
        "        total_loss = 0.0\n",
        "        total_loss1 = 0.0\n",
        "        total_loss2 = 0.0\n",
        "        total_acc = 0.0\n",
        "        total_iou = 0.0\n",
        "        process_start_time = time.time()\n",
        "        np.random.shuffle(train_data)\n",
        "        for j in range(num_batch):\n",
        "            begin_idx = j*BATCH_SIZE\n",
        "            end_idx = (j+1)*BATCH_SIZE\n",
        "            data_cells = train_data[begin_idx: end_idx,0]\n",
        "            batch_inputs = np.zeros((BATCH_SIZE,NUM_POINT,6),np.float32)\n",
        "            batch_proposal = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_dof_regression = np.zeros((BATCH_SIZE,1,6),np.float32)\n",
        "            batch_field = np.zeros((BATCH_SIZE,3,NUM_POINT,6),np.float32)\n",
        "            for cnt in range(BATCH_SIZE):\n",
        "                tmp_data = data_cells[cnt]\n",
        "                batch_inputs[cnt,:,:] = tmp_data['inputs_all'][0, 0]\n",
        "                batch_proposal[cnt,:] = np.squeeze(tmp_data['proposal'][0,0])\n",
        "                batch_dof_regression[cnt,:,:] = tmp_data['dof_regression'][0,0]\n",
        "                batch_field[cnt,:,:,:] = tmp_data['field'][0,0]\n",
        "            feed_dict = {ops['pointclouds_pl']: batch_inputs,\n",
        "                         ops['proposal_pl']: batch_proposal,\n",
        "                         ops['dof_regression_pl']: batch_dof_regression,\n",
        "                         ops['field_pl']: batch_field,\n",
        "                         ops['is_training_pl']: is_training}\n",
        "                    \n",
        "            summary, step, _, loss_val,loss1_val,loss2_val,acc_val,iou_val = sess.run([ops['merged'], ops['step'], \\\n",
        "                                 ops['train_op'],ops['loss'],ops['loss1'],ops['loss2'],ops['task_1_acc'],ops['iou']],feed_dict=feed_dict)\n",
        "            train_writer.add_summary(summary, step)\n",
        "            total_loss += loss_val\n",
        "            total_loss1 += loss1_val\n",
        "            total_loss2 += loss2_val\n",
        "            total_acc +=acc_val\n",
        "            total_iou +=iou_val \n",
        "            #print('loss: %f' % loss_val)\n",
        "        total_loss = total_loss * 1.0 / num_batch\n",
        "        total_loss1 = total_loss1 * 1.0 / num_batch\n",
        "        total_loss2 = total_loss2 * 1.0 / num_batch\n",
        "        total_acc = total_acc * 1.0 / num_batch\n",
        "        total_iou = total_iou * 1.0 / num_batch\n",
        "        process_duration = time.time() - process_start_time\n",
        "        examples_per_sec = num_data/process_duration\n",
        "        sec_per_batch = process_duration/num_batch\n",
        "        log_string('\\t%s: step: %f loss: %f duration time %.3f (%.1f examples/sec; %.3f sec/batch)' \\\n",
        "           % (datetime.now(),step,total_loss,process_duration,examples_per_sec,sec_per_batch))\n",
        "        log_string('\\tTask1 loss: %f,Task2 loss: %f'%(total_loss1,total_loss2))\n",
        "        log_string('\\tTask1 acc: %f,Task1 iou: %f'%(total_acc,total_iou))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    log_string('pid: %s'%(str(os.getpid())))\n",
        "    train()\n",
        "    LOG_FOUT.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}