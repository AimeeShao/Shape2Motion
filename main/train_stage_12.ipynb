{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "import math\n",
        "from datetime import datetime\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import socket\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
        "sys.path.append(BASE_DIR)\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
        "import provider\n",
        "import tf_util\n",
        "import scipy.io as sio\n",
        "\n",
        "# parse the arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu', type=int, default=2, help='GPU to use [default: GPU 0]')\n",
        "parser.add_argument('--model', default='model', help='Model name [default: model]')\n",
        "parser.add_argument('--stage_1_log_dir', default='stage_1_log', help='Log dir [default: log]')\n",
        "parser.add_argument('--stage_2_log_dir', default='stage_2_log', help='Log dir [default: log]')\n",
        "parser.add_argument('--num_point', type=int, default=4096, help='Point Number [default: 2048]')\n",
        "parser.add_argument('--max_epoch', type=int, default=101, help='Epoch to run [default: 201]')\n",
        "parser.add_argument('--batch_size', type=int, default=1, help='Batch Size during training [default: 32]')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
        "parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
        "parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')\n",
        "parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.7]')\n",
        "parser.add_argument('--stage',type=int,default=2,help='network stage')\n",
        "FLAGS = parser.parse_args()\n",
        "\n",
        "# training params\n",
        "EPOCH_CNT = 0\n",
        "BATCH_SIZE = FLAGS.batch_size\n",
        "NUM_POINT = FLAGS.num_point\n",
        "MAX_EPOCH = FLAGS.max_epoch\n",
        "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
        "GPU_INDEX = FLAGS.gpu\n",
        "MOMENTUM = FLAGS.momentum\n",
        "OPTIMIZER = FLAGS.optimizer\n",
        "DECAY_STEP = FLAGS.decay_step\n",
        "DECAY_RATE = FLAGS.decay_rate\n",
        "STAGE = FLAGS.stage\n",
        "\n",
        "# import the model\n",
        "MODEL = importlib.import_module(FLAGS.model) # import network module\n",
        "MODEL_FILE = os.path.join(ROOT_DIR, 'models', FLAGS.model+'.py')\n",
        "\n",
        "# set up logs\n",
        "if STAGE == 1:\n",
        "    LOG_DIR = FLAGS.stage_1_log_dir\n",
        "else:\n",
        "    LOG_DIR = FLAGS.stage_2_log_dir\n",
        "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
        "os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
        "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
        "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
        "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
        "\n",
        "BN_INIT_DECAY = 0.5\n",
        "BN_DECAY_DECAY_RATE = 0.5\n",
        "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
        "BN_DECAY_CLIP = 0.99\n",
        "\n",
        "# connect to network?\n",
        "HOSTNAME = socket.gethostname()\n",
        "\n",
        "def log_string(out_str):\n",
        "    LOG_FOUT.write(out_str+'\\n')\n",
        "    LOG_FOUT.flush()\n",
        "    print(out_str)\n",
        "\n",
        "def get_learning_rate(batch):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate        \n",
        "\n",
        "def get_learning_rate_stage_2(batch,base_learning_rate):\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "                        base_learning_rate,  # Base learning rate.\n",
        "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "                        DECAY_STEP,          # Decay step.\n",
        "                        DECAY_RATE,          # Decay rate.\n",
        "                        staircase=True)\n",
        "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
        "    return learning_rate        \n",
        "\n",
        "def get_bn_decay(batch):\n",
        "    bn_momentum = tf.train.exponential_decay(\n",
        "                      BN_INIT_DECAY,\n",
        "                      batch*BATCH_SIZE,\n",
        "                      BN_DECAY_DECAY_STEP,\n",
        "                      BN_DECAY_DECAY_RATE,\n",
        "                      staircase=True)\n",
        "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
        "    return bn_decay\n",
        "\n",
        "def train():\n",
        "    with tf.Graph().as_default():\n",
        "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
        "            if STAGE==1:\n",
        "                print('stage_1')\n",
        "                pointclouds_pl,labels_key_p,labels_direction,regression_direction, \\\n",
        "                                     regression_position,labels_type,simmat_pl,neg_simmat_pl= \\\n",
        "                                                                                        MODEL.placeholder_inputs_stage_1(BATCH_SIZE,NUM_POINT)\n",
        "                is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "                # Note the global_step=batch parameter to minimize. \n",
        "                # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "                batch_stage_1 = tf.Variable(0,name='stage1/batch')\n",
        "                bn_decay = get_bn_decay(batch_stage_1)\n",
        "                tf.summary.scalar('bn_decay', bn_decay)\n",
        "                print(\"--- Get model and loss\")\n",
        "                # Get model and loss \n",
        "                end_points,dof_feat,simmat_feat = MODEL.get_feature(pointclouds_pl, is_training_pl,STAGE,bn_decay=bn_decay)\n",
        "                pred_labels_key_p,pred_labels_direction,pred_regression_direction,\\\n",
        "                   pred_regression_position,pred_labels_type,pred_simmat,pred_conf_logits = \\\n",
        "                                                                       MODEL.get_stage_1(dof_feat,simmat_feat, is_training_pl,bn_decay=bn_decay)\n",
        "                task_1_loss,task_1_recall,task_1_acc,task_2_1_loss,task_2_1_acc,task_2_2_loss, \\\n",
        "                                    task_3_loss,task_4_loss,task_4_acc,task_5_loss,task_6_loss,loss = \\\n",
        "                       MODEL.get_stage_1_loss(pred_labels_key_p,pred_labels_direction,pred_regression_direction,pred_regression_position, \\\n",
        "                       pred_labels_type,labels_key_p,labels_direction,regression_direction,regression_position,labels_type,\\\n",
        "                       simmat_pl,neg_simmat_pl,pred_simmat,pred_conf_logits)\n",
        "                tf.summary.scalar('labels_key_p_loss', task_1_loss)\n",
        "                tf.summary.scalar('labels_key_p_recall', task_1_recall)\n",
        "                tf.summary.scalar('labels_key_p_acc', task_1_acc)\n",
        "                tf.summary.scalar('labels_direction_loss', task_2_1_loss)\n",
        "                tf.summary.scalar('labels_direction_acc', task_2_1_acc)\n",
        "                tf.summary.scalar('regression_direction_loss', task_2_2_loss)\n",
        "                tf.summary.scalar('regression_position_loss', task_3_loss)\n",
        "                tf.summary.scalar('labels_type_loss', task_4_loss)\n",
        "                tf.summary.scalar('labels_type_acc', task_4_acc)\n",
        "                tf.summary.scalar('loss', loss)\n",
        "\n",
        "                print(\"--- Get training operator\")\n",
        "                # Get training operator\n",
        "                learning_rate = get_learning_rate(batch_stage_1)\n",
        "                tf.summary.scalar('learning_rate', learning_rate)\n",
        "                if OPTIMIZER == 'momentum':\n",
        "                    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "                elif OPTIMIZER == 'adam':\n",
        "                    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "                train_op = optimizer.minimize(loss, global_step=batch_stage_1)\n",
        "            \n",
        "                # Add ops to save and restore all the variables.\n",
        "                saver = tf.train.Saver(max_to_keep=10)\n",
        "            elif STAGE==2:\n",
        "                print('stage_2')\n",
        "                pointclouds_pl,proposal_nx_pl,dof_mask_pl,dof_score_pl= MODEL.placeholder_inputs_stage_2(BATCH_SIZE,NUM_POINT)\n",
        "                is_training_feature= False\n",
        "                is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "                # Note the global_step=batch parameter to minimize. \n",
        "                # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
        "                batch_stage_2 = tf.Variable(0,name='stage2/batch_2')\n",
        "                bn_decay = get_bn_decay(batch_stage_2)\n",
        "                tf.summary.scalar('bn_decay', bn_decay)\n",
        "                print(\"--- Get model and loss\")\n",
        "                # Get model and loss \n",
        "                end_points,dof_feat,simmat_feat = MODEL.get_feature(pointclouds_pl, is_training_feature,STAGE,bn_decay=bn_decay)\n",
        "                pred_dof_score,all_feat = MODEL.get_stage_2(dof_feat,simmat_feat,dof_mask_pl,proposal_nx_pl,is_training_pl,bn_decay=bn_decay)\n",
        "                loss = MODEL.get_stage_2_loss(pred_dof_score,dof_score_pl,dof_mask_pl)\n",
        "                tf.summary.scalar('loss', loss)\n",
        "                print(\"--- Get training operator\")\n",
        "                # Get training operator\n",
        "                learning_rate = get_learning_rate(batch_stage_2)\n",
        "                tf.summary.scalar('learning_rate', learning_rate)\n",
        "                variables = tf.contrib.framework.get_variables_to_restore()\n",
        "                variables_to_resotre = [v for v in variables if v.name.split('/')[0]=='pointnet']\n",
        "                variables_to_train = [v for v in variables if v.name.split('/')[0]=='stage2']\n",
        "                if OPTIMIZER == 'momentum':\n",
        "                    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
        "                elif OPTIMIZER == 'adam':\n",
        "                    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "                train_op = optimizer.minimize(loss, global_step=batch_stage_2,var_list = variables_to_train)\n",
        "                # Add ops to save and restore all the variables.\n",
        "                saver = tf.train.Saver(var_list = variables_to_resotre)\n",
        "                saver2 = tf.train.Saver(max_to_keep=100)\n",
        "        # Create a session\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        config.allow_soft_placement = True\n",
        "        config.log_device_placement = False\n",
        "        sess = tf.Session(config=config)\n",
        "        # Add summary writers\n",
        "        merged = tf.summary.merge_all()\n",
        "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'), sess.graph)\n",
        "        \n",
        "        \n",
        "        # Init variables\n",
        "        if STAGE == 1:\n",
        "            init = tf.global_variables_initializer()\n",
        "            sess.run(init)\n",
        "        else:\n",
        "            init = tf.global_variables_initializer()\n",
        "            sess.run(init)\n",
        "            saver.restore(sess,'./stage_1_log/model100.ckpt')\n",
        "    if STAGE==1:\n",
        "            ops = {'pointclouds_pl': pointclouds_pl,\n",
        "               'labels_key_p': labels_key_p,\n",
        "               'labels_direction': labels_direction,\n",
        "               'regression_direction': regression_direction,\n",
        "               'regression_position': regression_position,\n",
        "               'labels_type': labels_type,\n",
        "               'simmat_pl': simmat_pl,\n",
        "               'neg_simmat_pl': neg_simmat_pl,\n",
        "               'is_training_pl': is_training_pl,\n",
        "               'pred_labels_key_p': pred_labels_key_p,\n",
        "               'pred_labels_direction': pred_labels_direction,\n",
        "               'pred_regression_direction': pred_regression_direction,\n",
        "               'pred_regression_position': pred_regression_position,\n",
        "               'pred_labels_type': pred_labels_type,\n",
        "               'pred_simmat': pred_simmat,\n",
        "               'pred_conf': pred_conf_logits,\n",
        "               'task_1_loss': task_1_loss,\n",
        "               'task_1_recall':task_1_recall,\n",
        "               'task_1_acc': task_1_acc,\n",
        "               'task_2_1_loss': task_2_1_loss,\n",
        "               'task_2_1_acc': task_2_1_acc,\n",
        "               'task_2_2_loss': task_2_2_loss,\n",
        "               'task_3_loss': task_3_loss,\n",
        "               'task_4_loss': task_4_loss,\n",
        "               'task_4_acc': task_4_acc,\n",
        "               'task_5_loss': task_5_loss,\n",
        "               'task_6_loss': task_6_loss,\n",
        "               'loss': loss,\n",
        "               'train_op': train_op,\n",
        "               'merged': merged,\n",
        "               'step': batch_stage_1,\n",
        "               'end_points': end_points}\n",
        "            for epoch in range(MAX_EPOCH):\n",
        "                log_string('**** TRAIN EPOCH %03d ****' % (epoch))\n",
        "                sys.stdout.flush()\n",
        "                train_one_epoch_stage_1(sess,ops,train_writer)\n",
        "                # Save the variables to disk.\n",
        "                if epoch % 2 == 0:\n",
        "                    model_ccc_path = \"model\"+str(epoch)+\".ckpt\"\n",
        "                    save_path = saver.save(sess, os.path.join(LOG_DIR, model_ccc_path))\n",
        "                    log_string(\"Model saved in file: %s\" % save_path)\n",
        "                elif STAGE==2:\n",
        "                    ops = {'pointclouds_pl': pointclouds_pl,\n",
        "                       'proposal_nx_pl': proposal_nx_pl,\n",
        "                       'dof_mask_pl': dof_mask_pl,\n",
        "                       'dof_score_pl': dof_score_pl,\n",
        "                       'pred_dof_score': pred_dof_score,\n",
        "                       'is_training_pl': is_training_pl,\n",
        "                       'loss': loss,\n",
        "                       'train_op': train_op,\n",
        "                       'merged': merged,\n",
        "                       'step': batch_stage_2,\n",
        "                       'all_feat':all_feat,\n",
        "                       'end_points': end_points}\n",
        "            for epoch in range(MAX_EPOCH):\n",
        "                log_string('**** TRAIN EPOCH %03d ****' % (epoch))\n",
        "                sys.stdout.flush()\n",
        "                train_one_epoch_stage_2(sess,ops,train_writer)\n",
        "                # Save the variables to disk.\n",
        "                if epoch % 2 == 0:\n",
        "                    model_ccc_path = \"model\"+str(epoch)+\".ckpt\"\n",
        "                    save_path = saver2.save(sess, os.path.join(LOG_DIR, model_ccc_path))\n",
        "                    log_string(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "\n",
        "def train_one_epoch_stage_1(sess, ops, train_writer):\n",
        "    is_training = True\n",
        "    permutation = np.random.permutation(5)\n",
        "    for i in range(len(permutation)/4):\n",
        "        load_data_start_time = time.time();\n",
        "        loadpath = './train_data/training_data_'+str(permutation[i*4]+1)+'.mat'\n",
        "        train_data = sio.loadmat(loadpath)['Training_data']\n",
        "        load_data_duration = time.time() - load_data_start_time\n",
        "        log_string('\\t%s: %s load time: %f' % (datetime.now(),loadpath,load_data_duration))\n",
        "        for j in range(3):\n",
        "            temp_load_data_start_time = time.time();\n",
        "            temp_loadpath = './train_data/training_data_'+str(permutation[i*4+j+1]+1)+'.mat'\n",
        "            temp_train_data = sio.loadmat(temp_loadpath)['Training_data']\n",
        "            temp_load_data_duration = time.time() - temp_load_data_start_time\n",
        "            log_string('\\t%s: %s load time: %f' % (datetime.now(),temp_loadpath,temp_load_data_duration))\n",
        "            train_data = np.concatenate((train_data,temp_train_data),axis = 0)\n",
        "            print(train_data.shape)\n",
        "        num_data = train_data.shape[0]\n",
        "        num_batch = num_data // BATCH_SIZE\n",
        "        total_loss = 0.0\n",
        "        total_task_1_loss = 0.0\n",
        "        total_task_1_acc = 0.0\n",
        "        total_task_1_recall = 0.0\n",
        "        total_task_2_1_loss = 0.0\n",
        "        total_task_2_1_acc = 0.0\n",
        "        total_task_2_2_loss = 0.0\n",
        "        total_task_3_loss = 0.0\n",
        "        total_task_4_loss = 0.0\n",
        "        total_task_4_acc = 0.0\n",
        "        total_task_5_loss = 0.0\n",
        "        total_task_6_loss = 0.0\n",
        "        process_start_time = time.time()\n",
        "        np.random.shuffle(train_data)\n",
        "        for j in range(num_batch):\n",
        "            begin_idx = j*BATCH_SIZE\n",
        "            end_idx = (j+1)*BATCH_SIZE\n",
        "            data_cells = train_data[begin_idx: end_idx,0]\n",
        "            batch_inputs = np.zeros((BATCH_SIZE,NUM_POINT,6),np.float32)\n",
        "            batch_labels_key_p = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_labels_direction = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_regression_direction = np.zeros((BATCH_SIZE,NUM_POINT,3),np.float32)\n",
        "            batch_regression_position = np.zeros((BATCH_SIZE,NUM_POINT,3),np.float32)\n",
        "            batch_labels_type = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_simmat_pl = np.zeros((BATCH_SIZE, NUM_POINT, NUM_POINT), np.float32)\n",
        "            batch_neg_simmat_pl = np.zeros((BATCH_SIZE, NUM_POINT, NUM_POINT), np.float32)\n",
        "            for cnt in range(BATCH_SIZE):\n",
        "                tmp_data = data_cells[cnt]\n",
        "                batch_inputs[cnt,:,:] = tmp_data['inputs_all'][0, 0]\n",
        "                batch_labels_key_p[cnt,:] = np.squeeze(tmp_data['core_position'][0,0])\n",
        "                batch_labels_direction[cnt,:] = np.squeeze(tmp_data['motion_direction_class'][0,0])\n",
        "                batch_regression_direction[cnt,:,:] = tmp_data['motion_direction_delta'][0,0]\n",
        "                batch_regression_position[cnt,:,:] = tmp_data['motion_position_param'][0,0]\n",
        "                batch_labels_type[cnt,:] = np.squeeze(tmp_data['motion_dof_type'][0,0])\n",
        "                tmp_simmat = tmp_data['similar_matrix'][0,0]\n",
        "                batch_simmat_pl[cnt,:,:] = tmp_simmat + tmp_simmat.T\n",
        "                tmp_neg_simmat = 1 - tmp_simmat\n",
        "                tmp_neg_simmat = tmp_neg_simmat - np.eye(NUM_POINT) \n",
        "                batch_neg_simmat_pl[cnt,:,:] = tmp_neg_simmat\n",
        "            feed_dict = {ops['pointclouds_pl']: batch_inputs,\n",
        "                         ops['labels_key_p']: batch_labels_key_p,\n",
        "                         ops['labels_direction']: batch_labels_direction,\n",
        "                         ops['regression_direction']: batch_regression_direction,\n",
        "                         ops['regression_position']: batch_regression_position,\n",
        "                         ops['labels_type']: batch_labels_type,\n",
        "                         ops['simmat_pl']: batch_simmat_pl,\n",
        "                         ops['neg_simmat_pl']: batch_neg_simmat_pl,\n",
        "                         ops['is_training_pl']: is_training}\n",
        "                 \n",
        "                    \n",
        "            summary, step, _, task_1_loss_val,task_1_recall_val,task_1_acc_val,task_2_1_loss_val,task_2_1_acc_val,task_2_2_loss_val, \\\n",
        "                                 task_3_loss_val,task_4_loss_val,task_4_acc_val,task_5_loss_val, \\\n",
        "                                 task_6_loss_val,loss_val = sess.run([ops['merged'], ops['step'], \\\n",
        "                                 ops['train_op'], ops['task_1_loss'], ops['task_1_recall'],ops['task_1_acc'],ops['task_2_1_loss'], \\\n",
        "                                 ops['task_2_1_acc'],ops['task_2_2_loss'],ops['task_3_loss'],ops['task_4_loss'], \\\n",
        "                                 ops['task_4_acc'],ops['task_5_loss'],ops['task_6_loss'],ops['loss']],feed_dict=feed_dict)\n",
        "            \n",
        "            train_writer.add_summary(summary, step)\n",
        "            total_loss += loss_val\n",
        "            total_task_1_loss += task_1_loss_val\n",
        "            total_task_1_acc += task_1_acc_val\n",
        "            total_task_1_recall += task_1_recall_val\n",
        "            total_task_2_1_loss += task_2_1_loss_val\n",
        "            total_task_2_1_acc += task_2_1_acc_val\n",
        "            total_task_2_2_loss += task_2_2_loss_val\n",
        "            total_task_3_loss += task_3_loss_val\n",
        "            total_task_4_loss += task_4_loss_val\n",
        "            total_task_4_acc += task_4_acc_val\n",
        "            total_task_5_loss += task_5_loss_val\n",
        "            total_task_6_loss += task_6_loss_val\n",
        "            #print('loss: %f' % loss_val)\n",
        "        total_loss = total_loss * 1.0 / num_batch\n",
        "        total_task_1_loss = total_task_1_loss * 1.0 / num_batch\n",
        "        total_task_1_acc = total_task_1_acc * 1.0 / num_batch\n",
        "        total_task_1_recall = total_task_1_recall * 1.0 / num_batch\n",
        "        total_task_2_1_loss = total_task_2_1_loss * 1.0 / num_batch\n",
        "        total_task_2_1_acc = total_task_2_1_acc * 1.0 / num_batch\n",
        "        total_task_2_2_loss = total_task_2_2_loss * 1.0 / num_batch\n",
        "        total_task_3_loss = total_task_3_loss * 1.0 / num_batch\n",
        "        total_task_4_loss = total_task_4_loss * 1.0 / num_batch\n",
        "        total_task_4_acc = total_task_4_acc * 1.0 / num_batch\n",
        "        total_task_5_loss = total_task_5_loss * 1.0 / num_batch\n",
        "        total_task_6_loss = total_task_6_loss * 1.0 / num_batch\n",
        "        process_duration = time.time() - process_start_time\n",
        "        examples_per_sec = num_data/process_duration\n",
        "        sec_per_batch = process_duration/num_batch\n",
        "        log_string('\\t%s: step: %f loss: %f duration time %.3f (%.1f examples/sec; %.3f sec/batch)' \\\n",
        "           % (datetime.now(),step,total_loss,process_duration,examples_per_sec,sec_per_batch))\n",
        "        log_string('\\t\\tTraining TASK 1 Mean_loss: %f' % total_task_1_loss)\n",
        "        log_string('\\t\\tTraining TASK 1 Accuracy: %f' % total_task_1_acc)\n",
        "        log_string('\\t\\tTraining TASK 1 Recall: %f' % total_task_1_recall)\n",
        "        log_string('\\t\\tTraining TASK 2_1 Mean_loss: %f' % total_task_2_1_loss)\n",
        "        log_string('\\t\\tTraining TASK 2_1 Accuracy: %f' % total_task_2_1_acc)\n",
        "        log_string('\\t\\tTraining TASK 2_2 Mean_loss: %f' % total_task_2_2_loss)\n",
        "        log_string('\\t\\tTraining TASK 3 Mean_loss: %f' % total_task_3_loss)\n",
        "        log_string('\\t\\tTraining TASK 4 Mean_loss: %f' % total_task_4_loss)\n",
        "        log_string('\\t\\tTraining TASK 4 Accuracy: %f' % total_task_4_acc)\n",
        "        log_string('\\t\\tTraining TASK 5 Mean_loss: %f' % total_task_5_loss)\n",
        "        log_string('\\t\\tTraining TASK 6 Mean_loss: %f' % total_task_6_loss)\n",
        "\n",
        "def train_one_epoch_stage_2(sess, ops, train_writer):\n",
        "    is_training = True\n",
        "    permutation = np.random.permutation(328)\n",
        "    for i in range(len(permutation)/4):\n",
        "        load_data_start_time = time.time();\n",
        "        loadpath = './train_data_stage_2/train_stage_2_data_'+str(permutation[i*4]+1)+'.mat'\n",
        "        train_data = sio.loadmat(loadpath)['Training_data']\n",
        "        load_data_duration = time.time() - load_data_start_time\n",
        "        log_string('\\t%s: %s load time: %f' % (datetime.now(),loadpath,load_data_duration))\n",
        "        for j in range(3):\n",
        "            temp_load_data_start_time = time.time();\n",
        "            temp_loadpath = './train_data_stage_2/train_stage_2_data_'+str(permutation[i*4+j+1]+1)+'.mat'\n",
        "            temp_train_data = sio.loadmat(temp_loadpath)['Training_data']\n",
        "            temp_load_data_duration = time.time() - temp_load_data_start_time\n",
        "            log_string('\\t%s: %s load time: %f' % (datetime.now(),temp_loadpath,temp_load_data_duration))\n",
        "            train_data = np.concatenate((train_data,temp_train_data),axis = 0)\n",
        "            print(train_data.shape)\n",
        "        \n",
        "        num_data = train_data.shape[0]\n",
        "        num_batch = num_data // BATCH_SIZE\n",
        "        total_loss = 0.0\n",
        "        process_start_time = time.time()\n",
        "        np.random.shuffle(train_data)\n",
        "        \n",
        "        for j in range(num_batch):\n",
        "            begin_idx = j*BATCH_SIZE\n",
        "            end_idx = (j+1)*BATCH_SIZE\n",
        "            data_cells = train_data[begin_idx: end_idx,0]\n",
        "            batch_inputs = np.zeros((BATCH_SIZE,NUM_POINT,6),np.float32)\n",
        "            batch_dof_mask = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_proposal_nx = np.zeros((BATCH_SIZE,NUM_POINT),np.int32)\n",
        "            batch_dof_score = np.zeros((BATCH_SIZE,NUM_POINT),np.float32)\n",
        "            for cnt in range(BATCH_SIZE):\n",
        "                tmp_data = data_cells[cnt]\n",
        "                batch_inputs[cnt,:,:] = tmp_data['inputs_all'][0, 0]\n",
        "                batch_dof_mask[cnt,:] = np.squeeze(tmp_data['dof_mask'][0,0])\n",
        "                batch_proposal_nx[cnt,:] = np.squeeze(tmp_data['proposal_nx'][0,0])\n",
        "                batch_dof_score[cnt,:] = np.squeeze(tmp_data['dof_score'][0,0])\n",
        "            feed_dict = {ops['pointclouds_pl']: batch_inputs,\n",
        "                         ops['proposal_nx_pl']: batch_proposal_nx,\n",
        "                         ops['dof_mask_pl']: batch_dof_mask,\n",
        "                         ops['dof_score_pl']: batch_dof_score,\n",
        "                         ops['is_training_pl']: is_training}\n",
        "                    \n",
        "            summary, step, _, loss_val = sess.run([ops['merged'], ops['step'], \\\n",
        "                                 ops['train_op'],ops['loss']],feed_dict=feed_dict)\n",
        "            train_writer.add_summary(summary, step)\n",
        "            total_loss += loss_val\n",
        "            #print('loss: %f' % loss_val)\n",
        "        total_loss = total_loss * 1.0 / num_batch\n",
        "        process_duration = time.time() - process_start_time\n",
        "        examples_per_sec = num_data/process_duration\n",
        "        sec_per_batch = process_duration/num_batch\n",
        "        log_string('\\t%s: step: %f loss: %f duration time %.3f (%.1f examples/sec; %.3f sec/batch)' \\\n",
        "           % (datetime.now(),step,total_loss,process_duration,examples_per_sec,sec_per_batch))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    log_string('pid: %s'%(str(os.getpid())))\n",
        "    train()\n",
        "    LOG_FOUT.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}